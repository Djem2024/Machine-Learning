{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c2366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import psutil\n",
    "import os\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3cb84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Preprocessing ( preprocessed assumption )\n",
    "# data = preprocessed data pickels/csvs here\n",
    "# X = data.data\n",
    "# y = data.target\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ac921",
   "metadata": {},
   "source": [
    "CONVERTING TO TENSORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c66e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple NN Implementation \n",
    "class SimpleNN(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hidden_layer_sizes=(8,), activation='relu'):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.layers = []\n",
    "        layer_sizes = [X.shape[1]] + list(self.hidden_layer_sizes) + [len(np.unique(y))]\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            W = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * 0.1\n",
    "            b = np.zeros((1, layer_sizes[i + 1]))\n",
    "            self.layers.append((W, b))\n",
    "        return self\n",
    "\n",
    "    def _activate(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        a = X\n",
    "        for i, (W, b) in enumerate(self.layers[:-1]):\n",
    "            z = np.dot(a, W) + b\n",
    "            a = self._activate(z)\n",
    "        z = np.dot(a, self.layers[-1][0]) + self.layers[-1][1]\n",
    "        return self.softmax(z)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f7a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch NN\n",
    "class TorchNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(TorchNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dff836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Inspired Robust NN (non-torch version)\n",
    "class RobustNN(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hidden_layer_sizes=(16, 16), activation='relu'):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.layers = []\n",
    "        layer_sizes = [X.shape[1]] + list(self.hidden_layer_sizes) + [len(np.unique(y))]\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            W = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * 0.1\n",
    "            b = np.zeros((1, layer_sizes[i + 1]))\n",
    "            self.layers.append((W, b))\n",
    "        return self\n",
    "\n",
    "    def _activate(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def dropout(self, x, drop_rate=0.2):\n",
    "        mask = np.random.binomial(1, 1 - drop_rate, size=x.shape)\n",
    "        return x * mask / (1 - drop_rate)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        a = X\n",
    "        for i, (W, b) in enumerate(self.layers[:-1]):\n",
    "            z = np.dot(a, W) + b\n",
    "            a = self._activate(z)\n",
    "            a = self.dropout(a)\n",
    "        z = np.dot(a, self.layers[-1][0]) + self.layers[-1][1]\n",
    "        return self.softmax(z)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ac36e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter and RAM Usage Function\n",
    "def model_info_sklearn(model):\n",
    "    if hasattr(model, 'coefs_') and hasattr(model, 'intercepts_'):\n",
    "        total_params = sum(w.size for w in model.coefs_) + sum(b.size for b in model.intercepts_)\n",
    "    elif hasattr(model, 'layers'):\n",
    "        total_params = sum(W.size + b.size for W, b in model.layers)\n",
    "    else:\n",
    "        total_params = 0\n",
    "    process = psutil.Process(os.getpid())\n",
    "    ram_usage = process.memory_info().rss / (1024 ** 2)  # in MB\n",
    "    return total_params, ram_usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41286372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared Grid Search Configuration\n",
    "shared_param_grid = {\n",
    "    'hidden_layer_sizes': [\n",
    "        (8,), (16,), (32,),\n",
    "        (8, 8), (16, 16), (32, 32),\n",
    "        (8, 16), (16, 8), (8, 32), (32, 8),\n",
    "        (16, 32), (32, 16), (8, 16, 8), (16, 8, 16)\n",
    "    ],\n",
    "    'activation': ['relu', 'tanh']\n",
    "}\n",
    "\n",
    "# Grid Search for All Models\n",
    "models = {\n",
    "    'MLPClassifier': MLPClassifier(max_iter=500, random_state=42),\n",
    "    'SimpleNN': SimpleNN(),\n",
    "    'RobustNN': RobustNN()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    gs = GridSearchCV(model, shared_param_grid, cv=3)\n",
    "    gs.fit(X_train, y_train)\n",
    "    pred = gs.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    params, ram = model_info_sklearn(gs.best_estimator_)\n",
    "    print(f\"{name} - Accuracy: {acc:.4f}, Total Parameters: {params}, RAM Usage: {ram:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
